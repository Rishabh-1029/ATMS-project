{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "5f21c075",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "a6017128",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import random\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as T\n",
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from tqdm import tqdm\n",
    "from tqdm import trange\n",
    "from torchvision.ops import box_iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "36bbd727",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrafficDataset(Dataset):\n",
    "    def __init__(self, img_dir, label_dir, transform = None, img_size=(512,512), max_samples = None):\n",
    "        self.img_dir = img_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.transform = transform\n",
    "        self.img_size = img_size\n",
    "        self.images = [img for img in os.listdir(img_dir) if img.endswith(('.jpg', '.png', '.jpeg'))]\n",
    "        \n",
    "        if max_samples is not None:\n",
    "            self.images = random.sample(self.images, min(max_samples, len(self.images)))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        img_name = self.images[index]\n",
    "        img_path = os.path.join(self.img_dir, img_name)\n",
    "        label_path = os.path.join(self.label_dir, img_name.replace('.jpg','.txt').replace('.png','.txt'))\n",
    "        \n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        h, w, _ = image.shape\n",
    "        \n",
    "        boxes = []\n",
    "        labels = []\n",
    "        if os.path.exists(label_path):\n",
    "            with open(label_path, 'r') as f:\n",
    "                for line in f.readlines():\n",
    "                    cls, x_c, y_c, bw, bh = map(float, line.strip().split())\n",
    "                    x_c *= w\n",
    "                    y_c *= h\n",
    "                    bw *= w\n",
    "                    bh *= h\n",
    "                    x1 = x_c - bw / 2\n",
    "                    x2 = x_c + bw / 2\n",
    "                    y1 = y_c - bh / 2\n",
    "                    y2 = y_c + bh / 2\n",
    "                    boxes.append([x1, y1, x2, y2])\n",
    "                    labels.append(int(cls))\n",
    "        \n",
    "        if not boxes:\n",
    "            return self.__getitem__((index+1) % len(self))\n",
    "                    \n",
    "        boxes = torch.tensor(boxes, dtype = torch.float32)\n",
    "        labels = torch.tensor(labels, dtype = torch.int64)\n",
    "        image = cv2.resize(image, self.img_size)\n",
    "        scale_x = self.img_size[0] / w\n",
    "        scale_y = self.img_size[1] / h\n",
    "        boxes[:, [0,2]]*=scale_x\n",
    "        boxes[:, [1,3]]*=scale_y\n",
    "        image = T.ToTensor()(image)\n",
    "        \n",
    "        target = {\n",
    "            \"boxes\": boxes,\n",
    "            \"labels\": labels\n",
    "        }\n",
    "        \n",
    "        return image, target        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "f325f971",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TrafficDataset(\n",
    "    img_dir = r\"C:\\Users\\Rishabh Surana\\Desktop\\ATMS project\\Custom model for Object Detection\\Road Image Dataset\\trafic_data\\train\\images\", \n",
    "    label_dir = r\"C:\\Users\\Rishabh Surana\\Desktop\\ATMS project\\Custom model for Object Detection\\Road Image Dataset\\trafic_data\\train\\labels\",\n",
    "    max_samples = 500\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "89372994",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dataset = TrafficDataset(\n",
    "    img_dir = r\"C:\\Users\\Rishabh Surana\\Desktop\\ATMS project\\Custom model for Object Detection\\Road Image Dataset\\trafic_data\\valid\\images\", \n",
    "    label_dir = r\"C:\\Users\\Rishabh Surana\\Desktop\\ATMS project\\Custom model for Object Detection\\Road Image Dataset\\trafic_data\\valid\\labels\",\n",
    "    max_samples = 300\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "0d30a653",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training images: 500\n",
      "Number of validation images: 300\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of training images: {len(train_dataset)}\")\n",
    "print(f\"Number of validation images: {len(valid_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "84f7be52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({5: 898, 4: 656, 13: 576, 17: 557, 10: 412, 18: 297, 11: 260, 9: 163, 15: 154, 19: 140, 2: 75, 3: 65, 7: 39, 20: 26, 16: 16, 8: 14, 0: 11, 1: 8, 14: 6, 12: 3})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "label_counter = Counter()\n",
    "for _, target in train_dataset:\n",
    "    label_counter.update(target['labels'].tolist())\n",
    "print(label_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "6877da89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image 0: shape = torch.Size([3, 512, 512]), target ={'boxes': tensor([[338.4000, 340.6222, 503.2000, 489.9556],\n",
      "        [275.2000, 282.3111, 334.4000, 403.2000],\n",
      "        [357.2000, 317.1555, 407.6000, 359.8222],\n",
      "        [ 68.4000, 261.6889, 176.4000, 392.5334],\n",
      "        [211.2000, 283.7333, 259.2000, 393.2444],\n",
      "        [262.0000, 338.4889, 275.6000, 366.9333],\n",
      "        [199.2000, 313.6000, 213.6000, 369.0667]]), 'labels': tensor([ 5, 11,  4,  4,  4,  9,  4])}\n",
      "Image 1: shape = torch.Size([3, 512, 512]), target ={'boxes': tensor([[414.0000, 243.2000, 510.8000, 499.2000],\n",
      "        [200.4000, 175.6445, 374.0000, 398.9333],\n",
      "        [ 52.8000,  68.2667, 164.8000, 210.4889],\n",
      "        [ 21.2000,  45.5111,  58.8000, 122.3111],\n",
      "        [  0.0000,  19.9111,  48.0000,  65.4222],\n",
      "        [109.2000,  16.3556, 132.4000,  32.0000],\n",
      "        [333.2000, 379.7333, 434.8000, 510.5778],\n",
      "        [164.8000, 333.5111, 280.0000, 468.6222],\n",
      "        [ 74.0000, 216.1778, 142.0000, 304.3556],\n",
      "        [110.0000, 184.8889, 149.2000, 264.5333],\n",
      "        [ 24.8000, 147.9111,  79.2000, 227.5556],\n",
      "        [  1.2000, 124.4445,  34.0000, 171.3778],\n",
      "        [  2.0000,  85.3333,  41.2000, 133.6889],\n",
      "        [ 73.2000, 147.2000,  94.8000, 208.3556],\n",
      "        [112.4000, 186.3111, 154.8000, 260.2667],\n",
      "        [168.8000, 121.6000, 192.8000, 177.0667],\n",
      "        [340.4000, 219.7333, 386.0000, 312.1778],\n",
      "        [202.8000, 140.0889, 254.8000, 192.7111],\n",
      "        [190.4000, 177.0667, 206.4000, 232.5333],\n",
      "        [184.4000, 159.2889, 196.4000, 193.4222],\n",
      "        [ 72.8000,  51.9111,  96.8000,  70.4000],\n",
      "        [ 63.2000,  38.4000,  90.4000,  66.8444],\n",
      "        [ 98.4000,  38.4000, 128.8000,  61.1556]]), 'labels': tensor([ 4,  4,  4,  4,  4,  4, 17,  5,  5, 17, 19,  5, 11, 17, 17, 17, 17,  5,\n",
      "         5, 14,  5,  5,  5])}\n",
      "Image 2: shape = torch.Size([3, 512, 512]), target ={'boxes': tensor([[  0.0000, 216.1778, 248.0000, 506.3111],\n",
      "        [376.4000, 255.2889, 422.0000, 390.4000],\n",
      "        [426.4000, 244.6222, 511.2000, 486.4000],\n",
      "        [406.0000, 236.8000, 456.4000, 263.8222],\n",
      "        [227.6000,  34.1333, 378.8000, 457.9556],\n",
      "        [  0.0000,   0.0000, 251.2000, 220.4444]]), 'labels': tensor([13, 13, 13,  4,  4,  4])}\n",
      "Image 3: shape = torch.Size([3, 512, 512]), target ={'boxes': tensor([[342.8000, 100.2667, 511.6000, 505.6000]]), 'labels': tensor([4])}\n",
      "Image 4: shape = torch.Size([3, 512, 512]), target ={'boxes': tensor([[ 11.3778, 171.2000,  71.1111, 198.4000],\n",
      "        [315.0222, 178.4000, 367.6444, 204.0000],\n",
      "        [477.1556, 181.6000, 489.9556, 192.8000]]), 'labels': tensor([17, 19, 17])}\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    img, tgt = train_dataset[i]\n",
    "    print(f\"Image {i}: shape = {img.shape}, target ={tgt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "c15ae8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(num_classes):\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "4eb8beab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Rishabh Surana\\Desktop\\ATMS project\\venv-gpu\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Rishabh Surana\\Desktop\\ATMS project\\venv-gpu\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "FasterRCNN(\n",
       "  (transform): GeneralizedRCNNTransform(\n",
       "      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
       "      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n",
       "  )\n",
       "  (backbone): BackboneWithFPN(\n",
       "    (body): IntermediateLayerGetter(\n",
       "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "      (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      (layer1): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer2): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer3): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (4): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (5): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer4): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (fpn): FeaturePyramidNetwork(\n",
       "      (inner_blocks): ModuleList(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (2): Conv2dNormActivation(\n",
       "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (layer_blocks): ModuleList(\n",
       "        (0-3): 4 x Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (extra_blocks): LastLevelMaxPool()\n",
       "    )\n",
       "  )\n",
       "  (rpn): RegionProposalNetwork(\n",
       "    (anchor_generator): AnchorGenerator()\n",
       "    (head): RPNHead(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (roi_heads): RoIHeads(\n",
       "    (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\n",
       "    (box_head): TwoMLPHead(\n",
       "      (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n",
       "      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    )\n",
       "    (box_predictor): FastRCNNPredictor(\n",
       "      (cls_score): Linear(in_features=1024, out_features=22, bias=True)\n",
       "      (bbox_pred): Linear(in_features=1024, out_features=88, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes = 22\n",
    "model = get_model(num_classes)\n",
    "device = torch.device(\"cuda\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "e4a026ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle = True, num_workers=0, collate_fn=lambda x: tuple(zip(*x)))\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=1, shuffle = False, num_workers=0, collate_fn=lambda x: tuple(zip(*x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "74fc0fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.001, momentum=0.9, weight_decay=0.0005)\n",
    "lr_schedular = torch.optim.lr_scheduler.StepLR(optimizer, step_size = 3, gamma = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "087a7d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, optimizer, data_loader, device, epoch):\n",
    "    print(\"\\n\\n ---Training ---\")\n",
    "    print(f\"Started training epoch {epoch+1}\")\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    loop = tqdm(enumerate(data_loader), total=len(data_loader), desc=f\"Epoch [{epoch+1}]\")\n",
    "    \n",
    "    for i, (images, targets) in loop:\n",
    "        images = [img.to(device) for img in images]\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        \n",
    "        loss_dict = model(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        loss_value = losses.item()\n",
    "        total_loss += loss_value\n",
    "        \n",
    "        loop.set_postfix({\n",
    "            \"Step\":f\"{i+1}/{len(data_loader)}\",\n",
    "            \"Loss\":f\"{loss_value:.4f}\",\n",
    "            \"AvgLoss\":f\"{total_loss/(i+1):.4f}\"\n",
    "        })\n",
    "           \n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    print(f\"\\n Epoch{epoch+1} completed. Avg Loss: {avg_loss:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "768ba130",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_best_model(model, acc, best_acc, path):\n",
    "    dir_name = os.path.dirname(path)\n",
    "    if dir_name != \"\":\n",
    "        os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    if acc >= best_acc:\n",
    "        torch.save(model.state_dict(), path)\n",
    "        print(f\"Best model saved at: {path} | Accuracy: {acc:.4f}\")\n",
    "        return acc\n",
    "    else:\n",
    "        print(f\"Not saved. Current acc ({acc:.4f}) < Best acc ({best_acc:.4f})\")\n",
    "    return best_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "0c3a27ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val_accuracy = 0.0\n",
    "num_epochs = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "6746614c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data_loader, device, iou_threshold=0.5, score_threshold=0.5):\n",
    "    print(f\"\\n\\n ---Evaluation @ threshold={score_threshold} ---\")\n",
    "    model.eval()\n",
    "    total_correct = 0\n",
    "    total_gt = 0\n",
    "    total_pred = 0\n",
    "\n",
    "    from torchvision.ops import box_iou\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, targets in data_loader:\n",
    "            images = [img.to(device) for img in images]\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "            \n",
    "            outputs = model(images)\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            for output in outputs:\n",
    "                if 'scores' in output:\n",
    "                    keep = output['scores'] > score_threshold\n",
    "                    output['boxes'] = output['boxes'][keep]\n",
    "                    output['labels'] = output['labels'][keep]\n",
    "                    output['scores'] = output['scores'][keep]\n",
    "\n",
    "            for pred, target in zip(outputs, targets):\n",
    "                pred_boxes = pred['boxes']\n",
    "                pred_labels = pred['labels']\n",
    "                true_boxes = target['boxes']\n",
    "                true_labels = target['labels']\n",
    "\n",
    "                if pred_boxes.numel() == 0 or true_boxes.numel() == 0:\n",
    "                    total_gt += len(true_labels)\n",
    "                    total_pred += len(pred_labels)\n",
    "                    continue\n",
    "\n",
    "                ious = box_iou(pred_boxes, true_boxes)\n",
    "                matched_indices = (ious > iou_threshold).nonzero(as_tuple=False)\n",
    "\n",
    "                matched_gt = set()\n",
    "                matched_pred = set()\n",
    "\n",
    "                for pred_idx, gt_idx in matched_indices:\n",
    "                    if (\n",
    "                        pred_labels[pred_idx] == true_labels[gt_idx]\n",
    "                        and gt_idx.item() not in matched_gt\n",
    "                        and pred_idx.item() not in matched_pred\n",
    "                    ):\n",
    "                        total_correct += 1\n",
    "                        matched_gt.add(gt_idx.item())\n",
    "                        matched_pred.add(pred_idx.item())\n",
    "\n",
    "                total_gt += len(true_labels)\n",
    "                total_pred += len(pred_labels)\n",
    "\n",
    "    precision = total_correct / total_pred if total_pred else 0\n",
    "    recall = total_correct / total_gt if total_gt else 0\n",
    "    f1 = (\n",
    "        2 * precision * recall / (precision + recall)\n",
    "        if (precision + recall) > 0\n",
    "        else 0\n",
    "    )\n",
    "\n",
    "    accuracy = total_correct / total_gt if total_gt else 0\n",
    "    print(f\"Eval Summary -> Precision: {precision:.4f} | Recall: {recall:.4f} | F1: {f1:.4f} | Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    return 1 - accuracy, accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9ee5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    for epoch in trange(num_epochs, desc=\"Training Epochs\"):\n",
    "        \n",
    "        try:\n",
    "            train_one_epoch(model, optimizer, train_loader, device, epoch)\n",
    "        except Exception as e:\n",
    "            print(f\"Error in training: {e}\") \n",
    "        lr_schedular.step()\n",
    "        model.to(\"cuda\")\n",
    "        torch.cuda.empty_cache()\n",
    "        val_loss, val_accuracy = evaluate(model, valid_loader, device)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1} | Val Loss: {val_loss:.4f} | Val Acc: {val_accuracy:.4f}\")\n",
    "        torch.cuda.empty_cache()\n",
    "        best_val_accuracy = save_best_model(model, val_accuracy, best_val_accuracy, \"CNN_obj_Traffic_detector_4.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a82e762",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 22  # include background\n",
    "device = torch.device(\"cpu\")\n",
    "model = get_model(num_classes)\n",
    "model.load_state_dict(torch.load(\"CNN_obj_Traffic_detector_4.pth\", map_location=device))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380d0275",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "def load_image(image_path, img_size=(512,512)):\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    orig_image = image.copy()\n",
    "    image = cv2.resize(image, img_size)\n",
    "    transform = T.ToTensor()\n",
    "    return transform(image), orig_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc68b087",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rescale_boxes(boxes, from_shape, to_shape):\n",
    "    scale_x = to_shape[1] / from_shape[1]\n",
    "    scale_y = to_shape[0] / from_shape[0]\n",
    "    boxes[:, [0, 2]] *= scale_x\n",
    "    boxes[:, [1, 3]] *= scale_y\n",
    "    return boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b7f376",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, image_tensor, threshold=0.5):\n",
    "    with torch.no_grad():\n",
    "        prediction = model([image_tensor.to(device)])[0]\n",
    "\n",
    "    keep = prediction['scores'] > threshold\n",
    "    boxes = prediction['boxes'][keep].cpu()\n",
    "    labels = prediction['labels'][keep].cpu()\n",
    "    scores = prediction['scores'][keep].cpu()\n",
    "    \n",
    "    return boxes, labels, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca2008a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_boxes(image, boxes, labels, scores):\n",
    "    for box, label, score in zip(boxes, labels, scores):\n",
    "        x1, y1, x2, y2 = map(int, box)\n",
    "        cv2.rectangle(image, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "        cv2.putText(image, f\"{label}: {score:.2f}\", (x1, y1 - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)\n",
    "    return image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9af3f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "image_path = r\"C:\\Users\\Rishabh Surana\\Desktop\\ATMS project\\test_data\\test_image\\image 4.jpg\"\n",
    "image_tensor, orig_image = load_image(image_path)\n",
    "\n",
    "boxes, labels, scores = predict(model, image_tensor, threshold=0.5)\n",
    "\n",
    "rescaled_boxes = rescale_boxes(boxes.clone(), from_shape=(512, 512), to_shape=orig_image.shape[:2])\n",
    "output_image = draw_boxes(orig_image, rescaled_boxes, labels, scores)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(output_image)\n",
    "plt.axis('off')\n",
    "plt.title(\"Detected Objects\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "b0e68057",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation data size : 500\n",
      "\n",
      "Model  1  : \n",
      "\n",
      "\n",
      " ---Evaluation @ threshold=0.4 ---\n",
      "Eval Summary -> Precision: 0.5746 | Recall: 0.5035 | F1: 0.5367 | Accuracy: 0.5035\n",
      "\n",
      "\n",
      " ---Evaluation @ threshold=0.5 ---\n",
      "Eval Summary -> Precision: 0.6614 | Recall: 0.4574 | F1: 0.5408 | Accuracy: 0.4574\n",
      "\n",
      "\n",
      " ---Evaluation @ threshold=0.6 ---\n",
      "Eval Summary -> Precision: 0.7302 | Recall: 0.4075 | F1: 0.5231 | Accuracy: 0.4075\n",
      "\n",
      "Model  2  : \n",
      "\n",
      "\n",
      " ---Evaluation @ threshold=0.4 ---\n",
      "Eval Summary -> Precision: 0.5454 | Recall: 0.4950 | F1: 0.5190 | Accuracy: 0.4950\n",
      "\n",
      "\n",
      " ---Evaluation @ threshold=0.5 ---\n",
      "Eval Summary -> Precision: 0.6433 | Recall: 0.4377 | F1: 0.5210 | Accuracy: 0.4377\n",
      "\n",
      "\n",
      " ---Evaluation @ threshold=0.6 ---\n",
      "Eval Summary -> Precision: 0.7404 | Recall: 0.3808 | F1: 0.5029 | Accuracy: 0.3808\n",
      "\n",
      "Model  3  : \n",
      "\n",
      "\n",
      " ---Evaluation @ threshold=0.4 ---\n",
      "Eval Summary -> Precision: 0.5722 | Recall: 0.5430 | F1: 0.5572 | Accuracy: 0.5430\n",
      "\n",
      "\n",
      " ---Evaluation @ threshold=0.5 ---\n",
      "Eval Summary -> Precision: 0.6450 | Recall: 0.5062 | F1: 0.5672 | Accuracy: 0.5062\n",
      "\n",
      "\n",
      " ---Evaluation @ threshold=0.6 ---\n",
      "Eval Summary -> Precision: 0.7161 | Recall: 0.4656 | F1: 0.5643 | Accuracy: 0.4656\n",
      "\n",
      "Model  4  : \n",
      "\n",
      "\n",
      " ---Evaluation @ threshold=0.4 ---\n",
      "Eval Summary -> Precision: 0.5715 | Recall: 0.5457 | F1: 0.5583 | Accuracy: 0.5457\n",
      "\n",
      "\n",
      " ---Evaluation @ threshold=0.5 ---\n",
      "Eval Summary -> Precision: 0.6504 | Recall: 0.5104 | F1: 0.5720 | Accuracy: 0.5104\n",
      "\n",
      "\n",
      " ---Evaluation @ threshold=0.6 ---\n",
      "Eval Summary -> Precision: 0.7159 | Recall: 0.4729 | F1: 0.5696 | Accuracy: 0.4729\n"
     ]
    }
   ],
   "source": [
    "num_classes = 22\n",
    "print(\"Validation data size : 500\")\n",
    "for i in range(4):\n",
    "    model = get_model(num_classes)\n",
    "    print(\"\\nModel \",i+1,\" : \")\n",
    "    m_path = f\"C:\\\\Users\\\\Rishabh Surana\\\\Desktop\\\\ATMS project\\\\Custom model for Object Detection\\\\CNN_obj_Traffic_detector_{i+1}.pth\"\n",
    "    model.load_state_dict(torch.load(m_path))\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    for thresh in [0.4, 0.5, 0.6]:\n",
    "        evaluate(model, valid_loader, device, score_threshold=thresh)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566af972",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deep_sort_realtime.deepsort_tracker import DeepSort"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
